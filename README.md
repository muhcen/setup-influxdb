# influxdb setup

for get token we should run this command

`openssl rand -hex 32`

and we get token and set in `.env` file

in the env file we have some property that is necessary for connect to `influxdb` image.

> - DOCKER_INFLUXDB_INIT_MODE=setup
> - DOCKER_INFLUXDB_INIT_HOST=influxdb
> - DOCKER_INFLUXDB_INIT_PORT=8086
> - INFLUXDB_URL=http://influxdb:8086
> - DOCKER_INFLUXDB_INIT_USERNAME=admin
> - DOCKER_INFLUXDB_INIT_PASSWORD=password
> - DOCKER_INFLUXDB_INIT_ORG=my-org
> - DOCKER_INFLUXDB_INIT_BUCKET=my-bucket
> - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=3b598732389a2440683c5c2fb87c68a9af7340a459049e28b90a1d3ef4fdf020
> - DOCKER_INFLUXDB_INIT_RETENTION=4d

## InfluxDB Automated Setup

**_The InfluxDB image contains some extra functionality to automatically bootstrap the system. This functionality is enabled by setting the DOCKER_INFLUXDB_INIT_MODE environment variable to the value setup when running the container. Additional environment variables are used to configure the setup logic:_**

**DOCKER_INFLUXDB_INIT_USERNAME:** The username to set for the system's initial super-user (Required).

**DOCKER_INFLUXDB_INIT_PASSWORD:** The password to set for the system's inital super-user (Required).

**DOCKER_INFLUXDB_INIT_ORG:** The name to set for the system's initial organization (Required).

**DOCKER_INFLUXDB_INIT_BUCKET:** The name to set for the system's initial bucket (Required).

**DOCKER_INFLUXDB_INIT_RETENTION:** The duration the system's initial bucket should retain data. If not set, the initial bucket will retain data forever.

**DOCKER_INFLUXDB_INIT_ADMIN_TOKEN:** The authentication token to associate with the system's initial super-user. If not set, a token will be auto-generated by the system.

> note: if we don't use automated setup we should first sign up in influxdb and influxdb return token for use in server and client.

## influxdb client

we use this package **`@influxdata/influxdb-client`** for connect to the influxdb node.

### What properties do we need?

- url (host address)
- token (ensure secure interaction between InfluxDB and clients)
- org (An organization is a workspace for a group of users)
- bucket (a named location where time series data is stored)

code example:

```
  const InfluxDBClient = new InfluxDB({
    url,
    token,
  });
  const queryApi = InfluxDBClient.getQueryApi(org);
  const writeApi = InfluxDBClient.getWriteApi(org, bucket);
```

## api

we write two api for read and write in influxdb.

```
import { Point } from '@influxdata/influxdb-client';

export function generateBTC(startPrice) {
  let currentPrice = startPrice;
  let currentDate = new Date();
  currentDate.setDate(currentDate.getDate() - 1);

  let records = [];
  for (let index = 0; index < 24 * 60 * 60; index++) {
    let priceChange = Math.floor(Math.random() * 100);

    if (Math.random() > 0.508) priceChange *= -1;
    currentPrice += priceChange;

    currentDate.setSeconds(currentDate.getSeconds() + 1);

    const point = new Point('btcusd')
      .tag('location', 'server')
      .tag('pairs', 'BTCUSD')
      .floatField('price', currentPrice)
      .timestamp(new Date(currentDate));

    records.push(point);
  }

  return records;
}

```

with this api we generate 24 _ 60 _ 60 random records for btcusd price.

```
  async readFromInflux() {
    try {
      const bucket = this.configService.get<string>(
        'DOCKER_INFLUXDB_INIT_BUCKET',
      );
      const start = '-1h';
      const end = 'now()';
      const field = 'price';
      const measurement = 'btcusd';
      const query = `
      from(bucket: "${bucket}")
      |> range(start: ${start}, stop:${end})
      |> filter(fn: (r) => r["_measurement"] == "${measurement}" and r["pairs"] == "${this.symbol}" and r["_field"] == "${field}")
      |> aggregateWindow(every: ${this.interval}, fn: first)
      |> fill(usePrevious: true)
      |> yield(name:"open")

      from(bucket: "${bucket}")
      |> range(start: ${start}, stop:${end})
      |> filter(fn: (r) => r["_measurement"] == "${measurement}" and r["pairs"] == "${this.symbol}" and r["_field"] == "${field}")
      |> aggregateWindow(every: ${this.interval}, fn: max)
      |> fill(usePrevious: true)
      |> yield(name:"high")

      from(bucket: "${bucket}")
      |> range(start: ${start}, stop:${end})
      |> filter(fn: (r) => r["_measurement"] == "${measurement}" and r["pairs"] == "${this.symbol}" and r["_field"] == "${field}")
      |> aggregateWindow(every: ${this.interval}, fn: min)
      |> fill(usePrevious: true)
      |> yield(name:"low")

      from(bucket: "${bucket}")
      |> range(start: ${start}, stop:${end})
      |> filter(fn: (r) => r["_measurement"] == "${measurement}" and r["pairs"] == "${this.symbol}" and r["_field"] == "${field}")
      |> aggregateWindow(every: ${this.interval}, fn: last)
      |> fill(usePrevious: true)
      |> yield(name:"close")

      from(bucket: "${bucket}")
      |> range(start: ${start}, stop:${end})
      |> filter(fn: (r) => r["_measurement"] == "${measurement}" and r["pairs"] == "${this.symbol}" and r["_field"] == "${field}")
      |> aggregateWindow(every: ${this.interval}, fn: sum)
      |> fill(usePrevious: true)
      |> yield(name:"volume")
      `;

      let rowsCached = {};
      let rows = [];

      for await (const { values, tableMeta } of this.queryApi.iterateRows(
        query,
      )) {
        const o = tableMeta.toObject(values);
        if (!rowsCached[o._time]) rowsCached[o._time] = [];
        rowsCached[o._time].push(o);
      }

      Object.values(rowsCached).forEach((item: any) => {
        if (item.length) {
          const row = {};

          row['time'] = item[0]._time;
          row['pair'] = item[0].pair;

          item.forEach((item) => (row[item.result] = item._value));

          rows.push(row);
        }
      });

      return rows;
    } catch (error) {
      console.log(error.message);
      throw new BadRequestException(error.message);
    }
  }
```

with this api we calculate open, close, high, low and volume for every 5 minutes candle.

.

# basic concept (important)

- all data in InfluxDB have timestamp column and the collection of field-key and field-value pairs and tag keys and tag values(Tags are optional).
- note: it’s generally a good idea to make use of tags because, unlike fields, tags are indexed.
- The measurement acts as a container for tags, fields, and the time column. a measurement is conceptually similar to a table.
- A retention policy describes how long InfluxDB keeps data (DURATION) and how many copies of this data is stored in the cluster (REPLICATION).
- a series is a collection of points that share a measurement, tag set, and field key.

- ### Timing is everything

- **In InfluxDB, a timestamp identifies a single point in any given data series. This is like an SQL database table where the primary key is pre-set by the system and is always time.**

# tag or field

Your queries should guide what data you store in tags and what you store in fields :

Store commonly-queried and grouping (group() or GROUP BY) metadata in tags **(like SQL)**.
Store data in fields if each data point contains a different value.
Store numeric values as fields (tag values only support string values).

> **wery important:**
>
> > to reduce memory consumption, consider storing high-cardinality values in field values rather than in tags or field keys.

> **note**: You cannot store more than one point with the same timestamp in a series. If you write a point to a series with a timestamp that matches an existing point, the field set becomes a union of the old and new field set

> note: In InfluxDB you don’t have to define schemas up front.

### series cardinality

The number of unique database, measurement, tag set, and field key combinations in an InfluxDB instance.

\***\*we shloud not use tag with high cardinality.\*\***

### point

A point represents a single data record that has four components: a measurement, tag set, field set, and a timestamp.

point example:

```
name: census
-----------------
time butterflies honeybees location scientist
2015-08-18T00:00:00Z 1 30 1 perpetua
```

.

**An InfluxDB database is similar to traditional relational databases. InfluxDB is a schemaless database which means it’s easy to add new measurements, tags, and fields at any time like nosql database.**

.

## important concept in influxdb

**batch**: A collection of data points in InfluxDB line protocol format, separated by newlines (0x0A).

**bucket**: A bucket is a named location where time series data is stored in InfluxDB(like database name).

**continuous query (CQ)**: An InfluxQL query that runs automatically and periodically within a database.

**database**: A logical container for users, retention policies, continuous queries, and time series data.

**duration**: The attribute of the retention policy that determines how long InfluxDB stores data. Data older than the duration are automatically dropped from the database.

**identifier**: Tokens that refer to continuous query names, database names, field keys, measurement names, retention policy names, subscription names, tag keys, and user names.

**replication factor**: The attribute of the retention policy that determines how many copies of data to concurrently store (or retain) in the cluster. Replicating copies ensures that data is available when a data node (or more) is unavailable.

**retention policy (RP)**: Describes how long InfluxDB keeps data (duration), how many copies of the data to store in the cluster (replication factor), and the time range covered by shard groups (shard group duration). RPs are unique per database and along with the measurement and tag set define a series.

**schema**: How the data are organized in InfluxDB.

**selector**: An InfluxQL function that returns a single point from the range of specified points.

**shard**: A shard contains the actual encoded and compressed data, and is represented by a TSM file on disk(like nosql database).

**High Availability and Clustering**: Clustering allows you to scale horizontally by adding more InfluxDB nodes to distribute the data and workload.

.

### join in influxDB

SQL JOINs aren’t available for InfluxDB is like an SQL table where the primary index is always pre-set to time. InfluxDB timestamps must be in UNIX epoch (January 1st, 1970 at 00:00:00 UTC).

.

### InfluxDB supports multiple query languages:

#### Flux

**_Flux_** is a data scripting language designed for querying, analyzing, and acting on time series data. Beginning with InfluxDB 1.8.0, Flux is available for production use along side InfluxQL.

#### InfluxQL

InfluxQL is an SQL-like query language for interacting with InfluxDB. It has been crafted to feel familiar to those coming from other SQL or SQL-like environments while also providing features specific to storing and analyzing time series data.

```
SELECT \* FROM "foodships" WHERE "planet" = 'Saturn' AND time > '2015-04-16 12:00:01'
```

.

## monitoring

for monitoring go to data explorer in localhost:8086 and query to database.

> we can use grafana for monitoring.

## Query InfluxDB with Flux

**_Pipe-forward operator(like linux):_**

-     |>

**_first we need bucket name:_**

- from(bucket:"telegraf/autogen")

**_Flux requires a time range when querying time series data:_**

-     from(bucket:"telegraf/autogen")
-     |> range(start: -1h)
- OR
-     |> range(start: -1h, stop: -10m)

when we set range between two time influxDB get back data between start and stop.

we can use Date:

-     |> range(start: 2018-11-05T23:30:00Z, stop: 2018-11-06T00:00:00Z)
  or we can remove stop:
-     |> range(start: -1y)

## output

of the flux query is number of series cardinality table. for every tag we have number of table. if we have 1 measurement and side tag with two value (sell and buy) and 3 fields, number of table is 1 _ 2 _ 3 = 6 and every table have \_field and \_value fields.
with this table we can draw the graph like trading view, in the graph we have 6 line because we have 6 table.

**_Pass your ranged data into the filter() function to narrow results based on data attributes or columns:_**

-     |> filter(fn: (r) => r._measurement == "trade" and r._field == "price")
- OR
-     |> filter(fn: (r) => r._measurement == "trade" and r._field == "total")

## output

if we use filter we select some table that we need like price or total. and influx return 2 table, because we have side tag and influx return 2 table with buy and sell side if we have 3 side influx return 3 table if we have 4 pair and 2 side influx return 8 table.

**_Use Flux’s yield() function to output the filtered tables as the result of the query:_**

-     |> yield()
- > note:influx CLI automatically assume a yield() function

or we can use this code for change the name of the result name:

-     |> yield(name: "low")

**_Window functions in InfluxDB allow you to group data into windows of time and then perform aggregations on those windows:_**

-     |> window(every: 5m)

**_Aggregate windowed data:_**

-     |> mean()

**_Add times to your aggregates:_**(As values are aggregated, the resulting tables do not have a \_time column because the records used for the aggregation all have different timestamps)

-     |> duplicate(column: "_stop", as: "_time")

**_Use the window() function with the inf parameter to gather all points into a single, infinite window:_**

-     |> window(every: inf)

create mean of every 5m in influx:

-     |> window(every: 5m)
      |> mean()
      |> duplicate(column: "_stop", as: "_time")
      |> window(every: inf)

## output

influx return 1 row for every 5 minute because we set interval to generate mean for every 5 minute. if we set 1 hour it is generate 1 row for every 1 hour.

**_use Helper functions instead of code above may (seem like a lot of coding):_**

-     |> aggregateWindow(every: 5m, fn: mean)

**_The group() function is used to specify the columns by which you want to group the data:_**

-     |> group(columns: ["side"])
  if we use side in group table divided into two part one of them is sell another is buy.
  if side have 4 type table divided into 4 table.

**_we can use sort and limit:_**

-     |> sort(columns: ["_value"])

  with sort we can sort buy value of the field or sort by tag.

-     |> limit(n: 10)
  use limit for return number of record in influx.

**_create function:_**

-     from(bucket: "db/rp")
        |> range(start: -10m)
        |> filter(fn: (r) => r._measurement == "mem" and r._field == "active")
        |> map(fn: (r) => ({r with _value: r._value / 1073741824}))
- OR
-     multiplyByX = (x, tables=<-) => tables
        |> map(fn: (r) => ({r with _value: r._value * x}))

      from(bucket: "my-bucket")
        |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
        |> filter(fn: (r) => r["_measurement"] == "trade")
        |> multiplyByX(x: 0.01)

in this two example we change value of the field.

**_Use the fill() function to replace null values with:_**

-     |> fill(usePrevious: true)
  if value is null influx use Previous value or we can use .value
-     |> fill(value: 0)
  with this code if field is null influx fill it with 0.

**_Use the first() or last() functions to return the first or last record in an input table:_**

-     |> first()
  first function return first record between two range (for example use for find open the candle)
-     |> last()
  last function return last record between two range (for example use for find close the candle)

**Pivot:**
In InfluxDB, the concept of "pivot" refers to reorganizing data from a columnar format into a row format.

-     from(bucket: "myBucket")
        |> range(start: -1d)
        |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")

in this example, The rowKey parameter specifies which columns should be used as row keys, while the columnKey parameter specifies which columns should be used as column keys.
in this example we convert all table to one table.

**Calculate percentages with Flux:** Calculating percentages from queried data is a common use case for time series data.

-     from(bucket: "my-bucket")
          |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
          |> filter(fn: (r) => r["_measurement"] == "trade")
          |> filter(fn: (r) => r["_field"] == "amount" or r["_field"] == "price")
          |> pivot(rowKey: ["_time"], columnKey: ["_field"], valueColumn: "_value")
          |> map(fn: (r) => ({r with _value: r.price / r.amount * 100.0}))

if we use pivot we access all field value and with this we can Calculate percentages with map function.

**cumulative sum:** Use the cumulativeSum() function to calculate a running total of values.

-     |> aggregateWindow(every: 5m, fn: sum)
      |> cumulativeSum()

this function add value to the all previous value. we should use aggregateWindow and every property for interval.

**Histograms:** provide valuable insight into the distribution of your data.

-     from(bucket: "my-bucket")
            |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
            |> filter(fn: (r) => r["_measurement"] == "trade" and r["_field"] == "price")
            |> histogram(bins: [8.0, 2000.0, 5000.0, 10000.0,20000.0])
  in this example we count number of price between quarter. influx return 5 row for every table.

**join:**

-     price = from(bucket: "my-bucket")
             |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
             |> filter(fn: (r) => r["_measurement"] == "trade" and r["_field"] == "price")

        total = from(bucket: "my-bucket")
             |> range(start: v.timeRangeStart, stop: v.timeRangeStop)
             |> filter(fn: (r) => r["_measurement"] == "trade" and r["_field"] == "total")

         join(tables: {price, total}, on: ["_time", "_stop", "_start", "location"])
             |> map(fn: (r) => ({_time: r._time, _value: r._value_price * r._value_total}))

  it is like sql join and it return price \* total for every timestamp and return one table.

**median:**
We can use this function to get the median.

-     |> median()

by default Output tables consist of a single row containing the calculated median.

-     |> median(method: "exact_selector")

  A selector method that returns the data point for which at least 50% of points are less than.

-     |> median(method: "exact_mean")
  Use the exact_mean method to return a single row per input table containing the average of the two values closest to the mathematical median of data in the table.

**movingAverage:**
For each row in a table, movingAverage() returns the average of the current value and previous values where n is the total number of values used to calculate the average.

-     |> movingAverage(n: 3)
  this function calculate 3 previous value and find mean of this value. and set this mean instead of value.
-     |> timedMovingAverage(every:2m,period:4m)
  what this function do? every 2 minutes calculate mean of the 4 minutes ago and set instead of \_value. if period is 4 minutes influxdb convert all row between time into one row and calculate mean.

**rate:** Use the derivative() function to calculate the rate of change between subsequent values or the aggregate.rate() function to calculate the average rate of change per window of time.

-     data
          |> derivative(unit: 1s)

**Conditionally transform column values with map() and :**

-     |> map(
          fn: (r) => ({r with
              level: if r._value >= 95.0000001 and r._value <= 100.0 then
                  "critical"
               else if r._value >= 85.0000001 and r._value <= 95.0 then
                  "warning"
               else if r._value >= 70.0000001 and r._value <= 85.0 then
                  "high"
               else
                  "normal",
               human_readable: if exists r._value then
                   "${r._field} is ${string(v: r._value)}."
               else
                   "${r._field} has no value.",
           }),
      )

## data type

**_All Flux data types are constructed from the following basic types:_**

- Boolean
- Bytes
- Duration
- Regular expression
- String
- Time
- Float
- Integer
- UIntegers
- Null

**_Flux composite types are types constructed from basic types. Flux supports the following composite types:_**

- Record
- Array
- Dictionary
- Function

**A dynamic type is a wrapper for data whose type is not known until runtime.**

### InfluxDB is not CRUD

- To update a point, insert one with the same measurement, tag set, and timestamp.
- You can drop or delete a series, but not individual points based on field values. As a workaround, you can search for the field value, retrieve the time, then DELETE based on the time field.
- You can’t update or rename tags yet - see GitHub issue #4157 for more information.
- To modify the tag of a series of points, find the points with the offending tag value, change the value to the desired one, write the points back, then drop the series with the old tag value.
- You can’t delete tags by tag key (as opposed to value) - see GitHub issue #8604.

.
.
.

\***\*Below is a list of some of those design insights that lead to tradeoffs:\*\***

- For the time series use case, we assume that if the same data is sent multiple times, it is the exact same data that a client just sent several times.

- Deletes are a rare occurrence. When they do occur it is almost always against large ranges of old data that are cold for writes.

- Updates to existing data are a rare occurrence and contentious updates never happen. Time series data is predominantly new data that is never updated.

- The vast majority of writes are for data with very recent timestamps and the data is added in time ascending order.

- Scale is critical. The database must be able to handle a high volume of reads and writes.

- Being able to write and query the data is more important than having a strongly consistent view.

- Many time series are ephemeral. There are often time series that appear only for a few hours and then go away, e.g. a new host that gets started and reports for a while and then gets shut down.

- No one point is too important.
